\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{fullpage} % changes the margin
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}



\begin{document}
\noindent
\large\textbf{Homework 2} \hfill \textbf{Yen-Lin Chen} \\
\normalsize ECE 5412 \hfill yc2253@cornell.edu \\
Fall 2019 \hfill Due: 10/30/19\\



\section*{Problem 37}

The AR2 model

\begin{equation}
y_k = -a_iy_{k-1}-a_2y_{k-2} + w_k 
\end{equation}
has the transfer function:

\begin{equation}
\frac{Y(z^{-1})}{U(z^{-1})} = \frac{z}{z^2+a_1z+a_2} 
\end{equation}

This model is stationary if the roots of $z^2+a_1z+a_2$ are in the unit circle of the complex plane, i.e. $|z_r| < 1$ where

\begin{equation}
z_r = \frac{-a_1}{2} \pm \frac{1}{2}\sqrt{a_1^2-4a_2}
\end{equation} 

First, consider the case where $a_1^2 < 4a_2$ and $z_r$ is complex. 

\begin{equation}
|z_r|^2 < 1 \iff \frac{a_1^2}{4} + \left(-\frac{a_1^2}{4} +a_2 \right) < 1 \iff a_2 < 1
\end{equation}

Second, consider the case where $a_1^2 > 4a_2$ and $x_r$ is real. 

\begin{equation}
|z_r| < 1 \iff \frac{-a_1}{2} + \frac{1}{2}\sqrt{a_1^2-4a_2} < 1 \quad \text{and} \quad \frac{-a_1}{2} - \frac{1}{2}\sqrt{a_1^2-4a_2} > -1
\end{equation}

\begin{equation}
|z_r| < 1 \iff a_1+a_2+1>0 \quad \text{and} \quad a_1-a_2-1<0
\end{equation}

The possible values of $a_1$ and $a_2$ are shown in Fig. 1 as the colored areas, where the blue and magenta regions correspond to complex and real poles respectively. 

\begin{figure}
	\begin{center}
		\includegraphics[width=4in]{p37.png}
		\caption{Problem 37: The stability triangle of the AR2 model. The blue and magenta areas correspond to complex and real poles respectively.}
	\end{center}
\end{figure}



\section*{Problem 38}
Choose $a_1 = -1.2$ and $a_2=0.7$ and the AR2 model is now
\begin{equation}
y_k = 1.2y_{k-1} - 0.7y_{k-2} + w_k
\end{equation}
with $y_0 = y_1 = 0$. We simulate 1000 observations of the system and compute the recursive least square (RLS) estimator for $a_1$ and $a_2$ using $Y = \Psi \theta + \epsilon$, where $Y = (y_3, y_4, \dots, y_{1002})'$, $\theta = (a_1, a_2)'$ and $\epsilon_i \sim N(0, I)$ iid. The trace is shown in the left plot of Fig. 2. The regression matrix $\Psi$ is the following

\begin{equation}
\Psi = \begin{bmatrix}
y_2 & y_1 \\
y_3 & y_4 \\ 
\vdots & \vdots\\
y_{1001} & y_{1000}
\end{bmatrix}
\end{equation}
Then, $\hat{\theta} = (\hat{a_1}, \hat{a_2})' = (\Psi' \Psi)^{-1} \Psi' Y$. We run 100 times of this estimation process to get the statistics of the estimators. The histograms of $\hat{a_1}$ and $\hat{a_2}$ are shown in the middle and right plots of Fig. 2. Using 100 simulations, we obtain $E[\hat{a_1}] = -1.1965$ with std $0.0221$ and $E[\hat{a_2}] = 0.7001$ with std $0.0219$. 

\begin{figure}
	\begin{center}
		\includegraphics[width=6.5in]{p38.png}
		\caption{Problem 38: (left) The trace of one simulation of 1000 observations using $a_1 = -1.2$, $a_2=0.7$ with $y_0 = y_1 = 0$. (middle) The histogram of $\hat{a_1}$. (right) The histogram of $\hat{a_2}$ from 100 simulations. }
	\end{center}
\end{figure}

To explore the effect of starting states $y_0$ and $y_1$, we fix $y_0=0$ and vary $y_1$ from $1$ to $1000$ and repeat the aforementioned process to compute the RLS estimators. The traces of $y_1 = 300, 600, 900$ are shown in Fig. 3 and $\hat{a_1}$ and $\hat{a_2}$ traces are shown in the bottom right plot of Fig. 3. The AR2 system stabilizes within 100 observations. In general, the RLS estimation works well, resulting in precise estimators of $a_1$ and $a_2$. However, we observe (relatively) larger estimation errors for $y_1 < 100$. For larger input difference, it propagates longer into the system so we have a better estimation of the parameters whereas smaller input difference disappears fast and the system is dominated by Gaussian noises later on. 

\begin{figure}
	\begin{center}
		\includegraphics[width=6.5in]{p38b.png}
		\caption{Problem 38: The effect of initial states. }
	\end{center}
\end{figure}


 

\section*{Problem 39}

The Akaike information criterion (AIC) is defined as 
\begin{equation}
\text{AIC} = -2\log{\hat{L}} + 2k
\end{equation}
where $k$ is the number of estimated parameters in the model and $\hat{L}$ is the maximum likelihood. Using the same notation, the Bayesian information criterion (BIC) is defined similarly, 
\begin{equation}
\text{BIC} = -2\log{\hat{L}} + (\log{n})2k
\end{equation}
where $n$ is the number of data points. The AIC for the ARMA model with parameter $p$ (order of AR, i.e. number of $a_i$'s), $q$ (order of MA, i.e. number of $b_i$'s) is 
\begin{equation}
\text{AIC}(arma(p,q)) = \log{(\hat{\sigma}^2(p,q))} + \frac{2(p+q)}{n}
\end{equation}
Similarly, for the BIC, 
\begin{equation}
\text{BIC}(arma(p,q)) = \log{(\hat{\sigma}^2(p,q))} + \frac{2(p+q)\log{n}}{n}
\end{equation}
where $\hat{\sigma}^2(p,q)$ is the variance of the residuals of the fitted ARMA model.
\begin{equation}
\hat{\sigma}^2(p,q) = \text{var}(Y - \hat{Y}) = \text{var}\left\{ \left[ I_{p+q} - \Psi (\Psi ' \Psi)^{-1} \Psi ' \right]Y \right\}
\end{equation}

In our case of AR model, $q = 0$. We simulate $n = 1002$ from an AR2 model using $a_1 = -1.2$ and $a_2=0.7$ with $y_0 = y_1 = 0$. The simulated data is shown in the top left panel of Fig. 3. We apply the RLS estimation for AR-k model where $k \in \{1,2,\dots, 300 \}$ and compare the AIC and BIC for all the models. We show the traces of residuals of AR-2 (middle left) and AR-100 models (bottom left) in Fig. 3.  

\begin{figure}
	\begin{center}
		\includegraphics[width=6.5in]{p39.png}
		\caption{Problem 39: (top left) The simulated data from an AR-2 system using $a_1 = -1.2$ and $a_2=0.7$ with $y_0 = y_1 = 0$. The traces of the residuals (middle left) using the AR-2 model and (bottom left) AR-100 model for RLS estimation. The panels on the right show the $\hat{\sigma}^2$, AIC and BIC of the AR-k models with the lowest value indicated as "x" which is the AR-1 model. }
	\end{center}
\end{figure}

Using the same simulated data, we estimated the AR-k parameters using RLS method. The variance of the residuals, the AIC and BIC are plotted in the right column of Fig. 3. We label the lowest value with "x" which turns out to be from the AR-1 model. Therefore, by AIC and BIC, the order of AR model is 1. 



\section*{Problem 43}

The engine is modeled by a combination of non-linear input model and AR-1 response. 
\begin{equation}
y_n = a_1u_{n-1} + a_2u_{n-1}^2 + a_3y_{n-1} + w_n
\end{equation}
Our goal is to use the vector $u$ and $y$ to construct a online least square estimation of the parameters $a_i$'s. In the MATLAB Simulink, the parameter estimations are updated each step; however, in our attempt to reproduce the MATLAB result, we extract the data from the Simulink model and do the online estimation ourselves. 

\begin{figure}
	\begin{center}
		\includegraphics[width=6.0in]{p43.png}
		\caption{Problem 43: (left) . }
	\end{center}
\end{figure}

In the Simulink online LS estimation, there is a forgetting parameter, $\lambda=10^{-4}$, suggesting that the LS model only takes into account the latest historical data to make LS estimation. Since the input pulse has period of 10 seconds, we have to use past $t$ seconds responses where $t > 10$. In Simulink, $\lambda=10^{-4}$ corresponds to about $15$ seconds of past data and exponential weights are also applied. 

On the other hand, in our attempt to make online LS estimation, we simply take past 15-second responses with equal weights to make estimation of $a_i$'s. Since the input pulse is a square wave, the regression matrix $\Psi$ might have linearly-dependent columns. The matrix $\Psi'\Psi$ might be close to singular matrix. We deal with such instability by using 
\begin{equation}
(\Psi'\Psi)^{-1} \approx (\Psi'\Psi + \epsilon^2 I)^{-1}
\end{equation}
where we set $\epsilon = 10^{-4}$. 

The input pulse and output response are shown on the left plots of Fig. 5. The online LS estimations of $a_i$ from our simple estimation and from Simulink are shown on the right panel of Fig. 5. This engine system is designed to change inertia at $t=100$ s, which affects the $a_1$ value. In both our and Simulink estimations, the estimated values of $a_1$ change abruptly at $t=100$ s, which signals the change in the engine inertia. 

The difference between our estimation and Simulink might be due to the followings, which can be improved for more precise estimation. First, we simply apply the past 15-second data to make online LS estimation. Using a forgetting parameter with longer data might improve the agreement. Second, we apply equal weights to the past data, which in real-life estimation can be improved by applying decaying weights to the past data. Finally, due to the equal weights, we are forced to apply $\epsilon^2 I$ to condition the $\Psi' \Psi$ matrix in case of close-to-singularity. However, if different weights are applied, $\Psi' \Psi$ will never be close to a singular matrix. Despite these simplifications in our online LS model estimation, we are still able to detect the change in $a_1$ at $t=100$ s. 

\section*{Problem 46}
The stochastic least square problem $Y = \Psi \theta + \epsilon$ where $\epsilon\sim N_N(0, \sigma^2I_N)$ has the estimator $\theta_*$: 
\begin{equation}
\theta_* = (\Psi ' \Psi)^{-1} \Psi ' Y 
\end{equation}
The estimator for noise variance $\hat{\sigma}^2$ is
\begin{equation}
\hat{\sigma}^2 = \frac{1}{N-n}(Y - \Psi\theta_*)'(Y - \Psi\theta_*)
\end{equation}
Plugging $\theta_*$ using Eq. (14). 
\begin{equation}
\hat{\sigma}^2 = \frac{1}{N-n}\left[(I - \Psi (\Psi ' \Psi)^{-1} \Psi ')Y \right]' \left[(I - \Psi (\Psi ' \Psi)^{-1} \Psi ')Y \right] = \frac{1}{N-n} (HY)'HY
\end{equation}
where $H = I - \Psi (\Psi ' \Psi)^{-1} \Psi '$ is symmetric and idenpotent ($H^2 = H$) and $\text{rank}(H) = \text{tr}(H) = N-n$. Define $Z = HY$. $\hat{\sigma}^2 = \frac{1}{N-n}Z'Z$.
\begin{equation}
\text{Cov}(Z) = H(\sigma^2 I)H' = \sigma^2 H
\end{equation}
\begin{equation}
E[\hat{\sigma}^2] = \frac{1}{N-n}E[Z'Z] = \frac{1}{N-n}\text{tr}(\text{Cov}(Z)) = \frac{\sigma^2}{N-n}(N-n) = \sigma^2
\end{equation}
Therefore, $\hat{\sigma}^2$ is an unbiased estimator for $\sigma^2$. 

By imposing $A\theta = 0$ where $A\in \mathbb{R}^{r\times n}$, we can rewrite the least square error with constraint by expanding the matrices as the following.
\begin{equation}
\begin{bmatrix}
Y\\
0_r
\end{bmatrix} = \begin{bmatrix}
\Psi \\
A
\end{bmatrix}\theta + \begin{bmatrix}
\epsilon \\
0_r
\end{bmatrix}
\end{equation}
The LS estimator $\hat{\theta}$ is now 
\begin{equation}
\hat{\theta} = \left(\begin{bmatrix}
\Psi' & A'
\end{bmatrix}\begin{bmatrix}
\Psi \\
A
\end{bmatrix} \right)^{-1} \begin{bmatrix}
\Psi' & A'
\end{bmatrix} \begin{bmatrix}
Y \\
0_r
\end{bmatrix} = \left(\Psi'\Psi + A'A \right)^{-1}\Psi'Y
\end{equation}

We can replace $\theta_*$ in Eq. (15) with $\hat{\theta}$ from Eq. (20) can get the new estimation of $\sigma^2$: 
\begin{equation}
\hat{\sigma_c}^2 = \frac{1}{N-n}(Y-\Psi\hat{\theta})'(Y-\Psi\hat{\theta})
\end{equation}

\begin{figure}
	\begin{center}
		\includegraphics[width=6.0in]{p46.png}
		\caption{Problem 46: (left) The trace of 1000 simulated $\hat{\sigma}^2$ without the constraint and the histogram. (right) The $\hat{\sigma_c}^2$ using Eq. (21) with respect to $r$. The error bar is the standard deviation of 1000 simulations of $\hat{\sigma_c}^2$. }
	\end{center}
\end{figure}

We run the simulation by varying $r$ using $N=1000$, $n=10$ and $\sigma^2=1$. $r=0$ is the case where there is no constraint. We compute 1000 estimators of $\sigma^2$ by using different $\Psi$ to get good statistics of $\hat{\sigma}^2$. The trace and histogram are shown on the left of Fig. 6. We repeat the estimation of $\hat{\sigma_c}^2$ using Eq. (21) for different size of $A\in \mathbb{R}^{r\times n}$ and explore the effect of imposing the constraint. The $\hat{\sigma_c}^2$ is increasing with $r$ which suggests that $\hat{\sigma_c}^2$ is no longer unbiased because it fails to taking into account the constraint by matrix $A$. 


Let $M = \left(\Psi'\Psi + A'A \right)^{-1}$. The residuals from the LS estimator $\hat{\theta}$ is 
\begin{equation}
r = \begin{bmatrix}
Y\\
0_r
\end{bmatrix} - \begin{bmatrix}
\Psi M \Psi'Y \\
AM\Psi' Y
\end{bmatrix} = \begin{bmatrix}
(I - \Psi M \Psi') \\
-AM\Psi' 
\end{bmatrix}Y
\end{equation}

\begin{equation}
\begin{split}
r'r & = Y'\left[I - 2\Psi M\Psi' + \Psi M\Psi' \Psi M\Psi' + \Psi MA'A M\Psi' \right]Y \\
& = Y'\left[I - 2\Psi M\Psi' + \Psi M(\Psi'\Psi + A'A)M\Psi' \right]Y \\
& = Y'(I - \Psi M\Psi')Y \\
& = Y' \left[I - \Psi\left(\Psi'\Psi + A'A \right)^{-1}\Psi' \right]Y
\end{split}
\end{equation}

Notice that $I - \Psi\left(\Psi'\Psi + A'A \right)^{-1}\Psi'$ is not an idenpotent matrix and might have large singular values, $\sigma_i$, depending on matrix $A$. This is the effect we observe in Fig. 6 where the estimator $\hat{\sigma_c}^2$ can be roughly written as
\begin{equation}
\hat{\sigma_c}^2 \approx \sigma^2 C(r)\sum_{i=1}^n \sigma_i^2
\end{equation}
where $C(r)$ is a constant that depends on $r$. If $r = 0$, $\sigma_i = 1$ $\forall i \in \{1,2,\dots, N-n \}$. Therefore, 

\begin{equation}
\hat{\sigma}^2 = \sigma^2 C(0)\sum_{i=1}^{N-n} \sigma_i^2 = \sigma^2(N-n)C(0)
\end{equation}
We can set $C(0) = 1/(N-n)$ to make $\hat{\sigma}^2$ an unbiased estimator. 



\section*{Problem 47}



 







\section*{Attachments}
	



\end{document}
